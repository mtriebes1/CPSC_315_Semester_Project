{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Weather Classification -- Sydney, Australia\n",
    "## Scott Campbell, Matthew Triebes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dataset description\n",
    "We sourced our dataset from Kaggle.com: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "When we examined the dataset, we realized that the dataset is a little too large to analyze. Because of this, we've decided to focus our efforts on examining the rain data out of Sydney Australia. That should limit the data to about 3400 instances which will make it a lot easier to analyze. \n",
    "\n",
    "With that taken into account, we’ve made separate files based on the 9am and 3pm data. Comparing those results should be interesting and we’ll see is there is much of a difference.\n",
    "\n",
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "source": [
    "## Loading the Dataset into Data Science Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = MyPyTable()\n",
    "\n",
    "table.load_from_file(\"Sydney_weather.csv\")\n",
    "\n",
    "x = table.get_column(\"MinTemp\", False)\n",
    "x_float = []\n",
    "y = table.get_column(\"Rainfall\", False)\n",
    "y_float = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if(x[i] != 'NA' and y[i] != 'NA'):\n",
    "        x_float.append(float(x[i]))\n",
    "        y_float.append(float(y[i]))"
   ]
  },
  {
   "source": [
    "### Discretizing Continuous Attributes\n",
    "\n",
    "The datatable has several attributes that are continuous variables that must first be discretized for use in the various classifiers, as well as the Random Forest Classifier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoffs and labels for the continuous attributes\n",
    "temp_cutoffs = [0, 5, 10, 15, 20, 25, 30]\n",
    "temp_labels = [kk+1 for kk in range(len(temp_cutoffs))]\n",
    "humidity_cutoffs = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "humidity_labels = [kk+1 for kk in range(len(humidity_cutoffs))]\n",
    "pressure_cutoffs = [950, 960, 970, 980, 990, 1000, 1010, 1020, 1030, 1040, 1050]\n",
    "pressure_labels = [kk+1 for kk in range(len(pressure_cutoffs))]\n",
    "# Get the attributes of interest from the datatable\n",
    "subdataset = table.get_multiple_columns([\"MinTemp\", \"MaxTemp\", \"WindGustDir\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"RainToday\"])\n",
    "new_table = MyPyTable(data=subdataset, column_names=[\"MinTemp\", \"MaxTemp\", \"WindGustDir\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"RainToday\"])\n",
    "# Remove all instances with NA\n",
    "new_table.remove_rows_with_missing_values()\n",
    "# Classify temps as continuous datat\n",
    "min_temp = new_table.get_column(\"MinTemp\")\n",
    "min_temp = myutils.classify_continuous_data(min_temp, temp_cutoffs, temp_labels, lower_inclusive_upper_exclusive=False)\n",
    "max_temp = new_table.get_column(\"MaxTemp\")\n",
    "max_temp = myutils.classify_continuous_data(max_temp, temp_cutoffs, temp_labels, lower_inclusive_upper_exclusive=False)\n",
    "humid9am = new_table.get_column(\"Humidity9am\")\n",
    "humid9am = myutils.classify_continuous_data(humid9am, humidity_cutoffs, humidity_labels, lower_inclusive_upper_exclusive=False)\n",
    "humid3pm = new_table.get_column(\"Humidity3pm\")\n",
    "humid3pm = myutils.classify_continuous_data(humid3pm, humidity_cutoffs, humidity_labels, lower_inclusive_upper_exclusive=False)\n",
    "pressure9am = new_table.get_column(\"Pressure9am\")\n",
    "pressure9am = myutils.classify_continuous_data(pressure9am, pressure_cutoffs, pressure_labels, lower_inclusive_upper_exclusive=False)\n",
    "pressure3pm = new_table.get_column(\"Pressure3pm\")\n",
    "pressure3pm = myutils.classify_continuous_data(pressure3pm, pressure_cutoffs, pressure_labels, lower_inclusive_upper_exclusive=False)\n",
    "windGust = new_table.get_column(\"WindGustDir\")\n",
    "rainToday = new_table.get_column(\"RainToday\")\n",
    "# Create the final table with the conditioning finished\n",
    "dataset = [ [min_temp[kk]] + [max_temp[kk]] + [humid9am[kk]] + [humid3pm[kk]] + [pressure9am[kk]] + [pressure3pm[kk]] + [windGust[kk]] + [rainToday[kk]] for kk in range(len(new_table.data))]\n",
    "table = MyPyTable(data=dataset, column_names=[\"MinTemp\", \"MaxTemp\", \"WindGustDir\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"RainToday\"])"
   ]
  },
  {
   "source": [
    "## Classification using a Decision Tree Classifier\n",
    "\n",
    "To begin, we chose to use a decision tree classifier and stratified k-fold validation using k=10. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\nAccuracy Results\n============================================================\nStratified 10-Fold Cross Validation\nDecision Tree: accuracy = 0.7950865600123658, error rate = 0.20491343998763423\n"
     ]
    }
   ],
   "source": [
    "k_cross_validation = 10 # The number of folds for the (stratifed) cross-validation\n",
    "\n",
    "# Create the X and y data\n",
    "X_train = table.get_multiple_columns([\"MinTemp\", \"MaxTemp\", \"WindGustDir\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\"])\n",
    "y_train = table.get_column(\"RainToday\")\n",
    "\n",
    "# Create the classifier and fit it using k-fold cross validation\n",
    "decisionTreeClassifier = MyDecisionTreeClassifier()\n",
    "correct_sum = 0\n",
    "# Get the train-test indices for cross-validation\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X_train, y_train, n_splits=k_cross_validation)\n",
    "# Run the fitting\n",
    "all_y_pred, all_y_actual = [], []\n",
    "for kk in range(k_cross_validation):\n",
    "    # Get the X,y train/test indices\n",
    "    train_indices, test_indices = train_folds[kk], test_folds[kk]\n",
    "    # Fit the classifier\n",
    "    X_test_indices, y_test_indices, y_test, y_test_pred = myevaluation.fit_classifier(decisionTreeClassifier, X_train, y_train, train_indices, test_indices, train_indices, test_indices, normalize_X=False)\n",
    "    # Fetch the y_test_actual values\n",
    "    y_test_actual = [y_train[kk] for kk in test_indices]\n",
    "    # Append these to their respective arrays\n",
    "    all_y_actual += y_test_actual\n",
    "    all_y_pred += y_test_pred\n",
    "    # Get the number correct\n",
    "    correct_sum += myutils.get_percent_correct(y_test_pred, y_test_actual)\n",
    "\n",
    "predictive_accuracy = correct_sum / k_cross_validation\n",
    "myutils.print_stratified_crossVal_results([predictive_accuracy], [\"Decision Tree\"], k_cross_validation, title=\"Accuracy Results\")"
   ]
  },
  {
   "source": [
    "### Notes on Decision Tree Classifier\n",
    "\n",
    "The decision tree classifier was able to do a good job of classifiying the dataset in the sense that its predictive accuracy is higher than the highest percentage of class labels in our binary classification problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Classification using a Random Forest Classifier\n",
    "\n",
    "\n",
    "# IN PROGRESS\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-a1cef75fa5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create a classifier and fit it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mrandForestClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrandForestClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremainder_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/CPSC_322_Semester_Project/mysklearn/myclassifiers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;31m# Now that the trees have been generated, go through and pick the M most accurate trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0msorted_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_trees_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0maccuracy_cutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the 3 random forest variables\n",
    "N = 20 # Total number of decision trees to generate\n",
    "M = 7 # Number of \"best\" trees ro keep\n",
    "F = 2 # Number of random attributes to select from\n",
    "\n",
    "# First, make sure everything is represented as a string\n",
    "X = table.get_multiple_columns([\"MinTemp\", \"MaxTemp\", \"WindGustDir\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\"])\n",
    "y = table.get_column(\"RainToday\")\n",
    "\n",
    "for row in range(len(X)):\n",
    "    y[row] = str(y[row])\n",
    "    for col in range(len(X[0])):\n",
    "        X[row][col] = str(X[row][col])\n",
    "\n",
    "# Generate a random stratified test set consisting of one third of the original data set, \n",
    "# with the remaining two thirds of the instances forming the \"remainder set\".\n",
    "test_X, test_y, remainder_X, remainder_y = myevaluation.random_stratified_train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Create a classifier and fit it\n",
    "randForestClassifier = MyRandomForestClassifier(N, M, F)\n",
    "randForestClassifier.fit(remainder_X, remainder_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}